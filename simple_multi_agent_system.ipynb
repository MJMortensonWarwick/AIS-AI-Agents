{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MJMortensonWarwick/AIS-AI-Agents/blob/main/simple_multi_agent_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTsGvsQdj7YM"
      },
      "source": [
        "# Simple Multi-Agent System\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7672x7QncOst"
      },
      "source": [
        "## DO THIS FIRST!!\n",
        "You will need to use a Hugging Face token to make this work. Follow these steps:\n",
        "1. Got to https://huggingface.co/\n",
        "2. Click \"Sign Up\" in the top right corner.\n",
        "3. Do the usual account sign up steps.\n",
        "4. Make sure you go to your email and click on the link to confirm your account.\n",
        "5. Once logged-in, click on your icon in the top right corner and select \"Access tokens\" (right at the bottom of the menu).\n",
        "6. Click \"+ Create new token\".\n",
        "7. Give your token a name and then scroll to the bottom to click \"Create\". You can ignore all the other options.\n",
        "8. Copy your token secret (\"hf_...\") and save it somewhere on your machine (e.g. Word or Notepad).\n",
        "9. Back in Colab, click on the key icon on the left hand side.\n",
        "10. Click on \"+ Add new secret\".\n",
        "11. Give the new secret the Name HF_TOKEN (please copy exactly this name).\n",
        "12. Paste in your token secret from step 8 as the Value.\n",
        "13. Make sure Notebook access is slid to the right. If done it will go blue and show a tick.\n",
        "14. Now select Runtime > Run all (it won't be quick)\n",
        "15. Read on!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKE8cNZNcf-q"
      },
      "source": [
        "## Tutorial Starts Here\n",
        "In this tutorial we will put together a simple multi-agent system designed to research AI.\n",
        "\n",
        "We will introduce a new tool for this system ... internet search via DuckDuckGo (a search engine).\n",
        "\n",
        "We will use a Small Languange Model ... [Microsoft Phi](https://azure.microsoft.com/en-us/products/phi). Phi is an example of a Small Language Model (SLM), although the largest of this family is 14 billion parameters (which feels pretty big to me but maybe that's because I'm old). SLMs are often used in the agent space as they can be downloaded (as we'll do here), trained on your own data and objectives, kept private, and can be run more easily on commercial hardware. Quite often we don't need all of the knowledge and skills that a LLM may have, so its a good trade-off.\n",
        "\n",
        "The model we use will be the mini version. Really its too small to do the task we will give it particularly well, but you could easily update this code to use a more powerful model on the HuggingFace model hub if you want to build your own agents (ideally look for \"Instruct\" models that are fine tuned to follow instructions). It will just take more time to run or require better hardware (e.g. Colab Pro).\n",
        "\n",
        "We start by setting every things up:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heZBlBPyXx4G"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U transformers accelerate bitsandbytes duckduckgo-search\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "from duckduckgo_search import DDGS\n",
        "from google.colab import userdata, output\n",
        "import os\n",
        "\n",
        "# If you haven't already added the secret in Colab, here's how:\n",
        "# Runtime > Secrets > Set 'HF_TOKEN' with your Hugging Face API token\n",
        "\n",
        "hf_api_token = userdata.get('HF_TOKEN')  # Make sure the secret is set in Colab\n",
        "\n",
        "# Set the token if it's retrieved correctly, else print an error\n",
        "if hf_api_token:\n",
        "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hf_api_token\n",
        "else:\n",
        "    print(\"Hugging Face API Token is missing. Please set it in Colab Secrets.\")\n",
        "\n",
        "# Load a small instruction-following model\n",
        "model_id = \"microsoft/Phi-3-mini-4k-instruct\" # this will be quite slow\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    dtype=torch.float16,\n",
        "    offload_buffers=True, # offloads stuff like attention scores to CPU to free up GPU\n",
        "    low_cpu_mem_usage=True\n",
        "    # low_cpu_mem_usage loads only parts of the module into RAM at a given moment in\n",
        "    # time, thus meaning we don't need to fill up RAM with the whole model\n",
        ")\n",
        "\n",
        "# we also define a pipeline as well will be repeatedly querying the LLM model\n",
        "pipe = pipeline(\n",
        "       \"text-generation\",\n",
        "       model=model,\n",
        "       tokenizer=tokenizer,\n",
        "       device_map=\"auto\",\n",
        "       dtype=torch.float16\n",
        "   )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uows3bWGl6Qd"
      },
      "source": [
        "There's quite a lot going on here, so let's discuss a little.\n",
        "\n",
        "\n",
        "*   We are using our HuggingFace to access their model hub (and other functions) to access the model we want.\n",
        "*   We specify the Microsoft Phi Mini model ... probably too small but makes the runtime acceptable in a tutorial like this.\n",
        "* We specify a tonkeniser to use to convert text to embeddings and back.\n",
        "* 'Bits and Bytes' is used to quantise the model. This is basically just a hack to make the model smaller (and therefore faster) without losing much of its key intelligence.\n",
        "* We then specify the model, using tricks like the quantisation (above), using the CPU to store things like weights to give the GPU more bandwidth and not loading the whole model into RAM (just loading parts when we need them). All of this is to make things run faster and in the Colab environment.\n",
        "* Lastly we specify a _pipeline_ which will allow us to interact with the model efficiently over multiple exchanges.\n",
        "\n",
        "\n",
        "\n",
        "Now we can start to define tools. Let's start with a common, but useful, example, the humble calculator:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define calculator tool\n",
        "def calculator(expression):\n",
        "    try:\n",
        "        return str(eval(expression))\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\""
      ],
      "metadata": {
        "id": "Jtn8Cs_O0Ppp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, an agent model can pass a calculation as text. The calculator will evaluate the calculation (using _eval_) and pass the results back as text.\n",
        "\n",
        "However, for this task (and to reduce the complexity of the tutorial) we will use just a single tool - DuckDuckGo search (via the DDGS function of the DuckDuckGo-Search Python package)."
      ],
      "metadata": {
        "id": "M8SvxB9H0QEY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8eXZaIRdrxW"
      },
      "outputs": [],
      "source": [
        "# DuckDuckGo tool\n",
        "def search_duckduckgo(query): # pass user query to DuckDuckGo\n",
        "    with DDGS() as ddgs:\n",
        "\n",
        "        # query using the user query and get top 3 results\n",
        "        results = ddgs.text(query, max_results=3)\n",
        "\n",
        "    # combine the top three results (just the meta description)\n",
        "    return \"\\n\".join([r[\"body\"] for r in results])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buuivwjKmdsF"
      },
      "source": [
        "With our tool setup we can now create our agent. We will follow the OOP-style (Object Orientated Programming) approach of creating an object from a class template. The idea of this is that this template (_ResearchAgent_) can be used to create multiple objects ... in this case agents that are tasked with performing different types of research."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAv_OAoLeFRD"
      },
      "outputs": [],
      "source": [
        "class ResearchAgent:\n",
        "    # rules to apply when instantiating an object of this class\n",
        "    def __init__(self, name, goal, llm, tools):\n",
        "        self.name = name # name of this object/agent\n",
        "        self.goal = goal # the agent's goal\n",
        "        self.llm = llm # the LLM we installed\n",
        "        self.tools = tools # the tools it can use (just DuckDuckGo in this case)\n",
        "\n",
        "    def run_task(self, query):\n",
        "        # extract context by using the search tool\n",
        "        context = self.tools[\"search\"](query)\n",
        "\n",
        "        # prompt template\n",
        "        prompt = f\"\"\"\n",
        "          You are a {self.name}.\n",
        "          Your goal is: {self.goal}\n",
        "          Using the following information from web search:\n",
        "          {context}\n",
        "          Write a comprehensive and professional report on the goal.\n",
        "          Please write no more than 200 words.\n",
        "          Do not repeat yourself.\n",
        "        \"\"\"\n",
        "\n",
        "        # rules for how to create the response\n",
        "        response = self.llm(prompt, # the prompt above\n",
        "                            # only return the answer not the prompt (full text)\n",
        "                            return_full_text=False,\n",
        "                            # maximum number of tokens to return\n",
        "                            max_new_tokens=256,\n",
        "                            # sample when decoding the response (tokens -> to text)\n",
        "                            # this allows the decoder to be a bit more creative when\n",
        "                            # generating the output by referring to the whole probability\n",
        "                            # distribution of the output rather than just the most likely.\n",
        "                            do_sample=True,\n",
        "                            # temperature controls how much creativity the LLM can have\n",
        "                            # higher is more creativity\n",
        "                            # [0] means return just the main response\n",
        "                            # extract from this the \"generated_text\" item\n",
        "                            temperature=0.8)[0][\"generated_text\"]\n",
        "        return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtVQ_hCuoJcV"
      },
      "source": [
        "Now we have set up the Class template we can create an object of this type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfNCJtGjdUwn"
      },
      "outputs": [],
      "source": [
        "# Set up agent and tools\n",
        "\n",
        "# first create the tools list - just one tool\n",
        "tools = {\"search\": search_duckduckgo}\n",
        "\n",
        "# now create an object of the ResearchAgent Class template. As per the __init__ rules,\n",
        "# it must have a name, a goal, an LLM connection (our pipeline) and the toolset.\n",
        "agent1 = ResearchAgent(\n",
        "    name=\"AI Trend Analyst\",\n",
        "    goal=\"Summarize the top 2024 AI breakthroughs\",\n",
        "    llm=pipe, # connect it to the pipeline we created earlier\n",
        "    tools=tools\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk6wgR_hoga7"
      },
      "source": [
        "And then let's put the agent to work!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-9K2YoKeTmr"
      },
      "outputs": [],
      "source": [
        "# Run the agent\n",
        "agent1_report = agent1.run_task(\"AI breakthroughs in 2024\")\n",
        "output.clear()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sadly lots of warning messages I can't supress ... but they are harmless. Let's see the report:"
      ],
      "metadata": {
        "id": "CoateEaYzRkR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6FXyn66XS7H"
      },
      "outputs": [],
      "source": [
        "print(\"=== FINAL REPORT ===\\n\")\n",
        "print(agent1_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFR43gK3oqeu"
      },
      "source": [
        "Even with a relatively small model the response should be quite good! Now let's build some more agents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYPIuJlHgcrJ"
      },
      "outputs": [],
      "source": [
        "agent2 = ResearchAgent(\n",
        "    name=\"AI Data Scientist\",\n",
        "    goal=\"Identify breakthrough AI technologies and research papers published in 2024\",\n",
        "    llm=pipe,\n",
        "    tools={\"search\": search_duckduckgo}\n",
        ")\n",
        "\n",
        "agent3 = ResearchAgent(\n",
        "    name=\"AI Ethics Officer\",\n",
        "    goal=\"Review ethical concerns and AI regulations that emerged in 2024\",\n",
        "    llm=pipe,\n",
        "    tools={\"search\": search_duckduckgo}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyJ1Q_pmozhS"
      },
      "source": [
        "Now we have two more agents doing slightly different searches to support slightly different goals. We should get three unique reports!\n",
        "\n",
        "However, reading three reports sounds very boring. What we really want is another agent that can summarise all three individual reports into just one report. Because AI should help us be more lazy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tk7axDAYgzb9"
      },
      "outputs": [],
      "source": [
        "class SummariserAgent:\n",
        "    def __init__(self, name, role, llm):\n",
        "        self.name = name\n",
        "        self.role = role\n",
        "        self.llm = llm\n",
        "\n",
        "    def summarise_reports(self, reports):\n",
        "        # Combine the reports from other agents\n",
        "        combined_reports = \"\\n\".join(reports)\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "          Your goal is to summarize the following reports into a cohesive, professional summary:\n",
        "\n",
        "          {combined_reports}\n",
        "\n",
        "          The report should be at least three paragraphs in length and should cover each of the individual reports sents to you.\n",
        "        \"\"\"\n",
        "\n",
        "        response = self.llm(prompt,\n",
        "                              return_full_text=False,\n",
        "                              # double the amount of tokens 256 for the research agents\n",
        "                              # 512 for our summariser\n",
        "                              max_new_tokens=512,\n",
        "                              do_sample=True,\n",
        "                              temperature=0.8)[0][\"generated_text\"]\n",
        "        return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWccY4ldpw1t"
      },
      "source": [
        "And again we can create an object of this Class template:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acCABtlNg7_n"
      },
      "outputs": [],
      "source": [
        "summariser = SummariserAgent(\n",
        "    name=\"AI Research Summarizer\",\n",
        "    role=\"Senior AI Research Analyst\",\n",
        "    llm=pipe\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpwSQBY7p4KC"
      },
      "source": [
        "Now we can call research agents #2 and #3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UV6WLLNzhCER"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache() # empty the cache - rolling memory in RAM to avoid overload\n",
        "\n",
        "# hashed out as we ran this earlier\n",
        "\n",
        "# Task 1: AI Researcher\n",
        "#agent1_report = agent1.run_task(\"AI breakthroughs in 2024\")\n",
        "#print(f\"\\n[{agent1.name}] Report:\\n{agent1_report}\\n\")\n",
        "\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# Task 2: AI Data Scientist\n",
        "agent2_report = agent2.run_task(\"AI research papers in 2024\")\n",
        "output.clear()\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Task 3: AI Ethics Officer\n",
        "agent3_report = agent3.run_task(\"AI ethics and regulations in 2024\")\n",
        "output.clear()\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHAF7Tx2c7s1"
      },
      "outputs": [],
      "source": [
        "print(f\"[{agent2.name}] Report:\\n{agent2_report}\\n\") # print report\n",
        "print(f\"\\n[{agent3.name}] Report:\\n{agent3_report}\\n\") # print report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWQuvp5pqRHe"
      },
      "source": [
        "Again, decent looking work. Let's summarise!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWTYkhCz_kYz"
      },
      "outputs": [],
      "source": [
        "# Summarise all the reports\n",
        "reports = [agent1_report, agent2_report, agent3_report]\n",
        "final_report = summariser.summarise_reports(reports)\n",
        "output.clear()\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2l8aNzpdBjE"
      },
      "outputs": [],
      "source": [
        "print(f\"Summary Report:\\n{final_report}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JihtpVVQqXUj"
      },
      "source": [
        "A bit slow, but a good overall summary!\n",
        "\n",
        "### __TASKS__:\n",
        "1. Try modifying the roles and prompts of the agents to explore another area (e.g. your area of specialisation). How well does it perform? How might we better instruct the _SummariserAgent_ to create the sort of output we want?\n",
        "2. Can you create another agent that checks the work of the researchers and asks them to rewrite if the response is not good? What would this agent class look like? How could we create the flow of work to complete these checks? You don't necessarily need to write this code in full but think about what it would look like."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}